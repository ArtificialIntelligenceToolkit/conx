{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dreams\n",
    "\n",
    "Work in progress. Attempting to turn a keras model into a conx model.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"monalisa.jpg\" width=\"200px\"></td>\n",
    "        <td><img src=\"monalisa-generated.png\" width=\"200px\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications import inception_v3\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = \"monalisa.jpg\"\n",
    "result_prefix = \"monoalisa-generated.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the names of the layers for which we try to maximize activation, as well as their weight in the final loss we try to maximize.\n",
    "\n",
    "You can tweak these setting to obtain new visual effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'features': {\n",
    "        'mixed2': 0.2,\n",
    "        'mixed3': 0.5,\n",
    "        'mixed4': 2.,\n",
    "        'mixed5': 1.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures\n",
    "    # into appropriate tensors.\n",
    "    img = load_img(image_path)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        factors = (1, 1,\n",
    "                   float(size[0]) / img.shape[2],\n",
    "                   float(size[1]) / img.shape[3])\n",
    "    else:\n",
    "        factors = (1,\n",
    "                   float(size[0]) / img.shape[1],\n",
    "                   float(size[1]) / img.shape[2],\n",
    "                   1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('..Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "K.set_learning_phase(0)\n",
    "\n",
    "# Build the InceptionV3 network with our placeholder.\n",
    "# The model will be loaded with pre-trained ImageNet weights.\n",
    "model = inception_v3.InceptionV3(weights='imagenet',\n",
    "                                 include_top=False)\n",
    "dream = model.input\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conx, version 3.4.0\n"
     ]
    }
   ],
   "source": [
    "from conx import import_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = import_keras_model(model, \"Inception V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clayer in network.layers:\n",
    "    if clayer.kind() == \"hidden\":\n",
    "        clayer.visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image(base_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.dataset._inputs = [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.layers[0].visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['base/js/namespace'], function(Jupyter) {\n",
       "    Jupyter.notebook.kernel.comm_manager.register_target('conx_svg_control', function(comm, msg) {\n",
       "        comm.on_msg(function(msg) {\n",
       "            var data = msg[\"content\"][\"data\"];\n",
       "            var images = document.getElementsByClassName(data[\"class\"]);\n",
       "            for (var i = 0; i < images.length; i++) {\n",
       "                if (data[\"href\"]) {\n",
       "                    images[i].setAttributeNS(null, \"href\", data[\"href\"]);\n",
       "                }\n",
       "                if (data[\"src\"]) {\n",
       "                    images[i].setAttributeNS(null, \"src\", data[\"src\"]);\n",
       "                }\n",
       "            }\n",
       "        });\n",
       "    });\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2000\" id=\"Inception V3\" image-rendering=\"pixelated\" viewBox=\"0 0 350 1830\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "    <defs>\n",
       "        <marker id=\"arrow\" markerHeight=\"10\" markerUnits=\"strokeWidth\" markerWidth=\"10\" orient=\"auto\" refX=\"9\" refY=\"3\">\n",
       "          <path d=\"M0,0 L0,6 L9,3 z\" fill=\"black\"/>\n",
       "        </marker>\n",
       "    </defs><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"15\" text-anchor=\"middle\" x=\"175.0\" y=\"12.5\">Inception V3</text><rect height=\"202\" style=\"fill:none;stroke:black;stroke-width:2\" width=\"135\" x=\"107.5\" y=\"29\"/><image class=\"Inception V3_mixed10\" height=\"200\" href=\"data:image/gif;base64,R0lGODdhFAAeAIcAAP78/QAATAAA8gAA0AAAbQAAr+jo/wAAj1VV/6am/9XV/0hI/4eH/7a2/8fH/ygo/zQ0/5SU/xgY/3d3/2dn/wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAFAAeAEAI/wABCHTggAEFBQ0ATBAwYIEDBwkMAABQ4ECEBxAAaASAAIBHAAoALBgQYECAAgweIADAsqXLBAgmJABQIMABCAYAAHAgQMAEAECBBghA4IADAwCSKl3KVKmAAAcKAJhKtWoDAFgdRAAAwECAAAwARChAAAKABAIEBFhbQACAAwkmBJhb4ICAAQcEHAjAl+8AAwACCwbgAIDhw4gjCHgAAIAAAA4AAGAAoHKDAAkENDAAgEABBQBCix4NQAEAAAgICADQAIBrAAooKDAAoDaAAQQmANjNWwCA38CDD6CwgMCBBAECLADAfMEDCAIIEChAYcGDANgXAAgQgECA7xIQFP9oAACAgAMCBhwIMACA+/fw3zMAAEABAAEKAOjfr38BA4AABDoAUNDggwQAFCpEMODAAgAAEAwAwADARQQFACRAAABAAwMUAIwEUECBhAECEgQIMGACAJgxZSYAAIBAgAIFAgyQ0ADAT6A/DUAAAMBAAAECCCigAMDpU6gDAEydWsAAAKwGIgDg2hWBAwYCAEAAIAECgAIA1K5li0AAgAESAMxNIIEAALwGACgAAMBAhACBAwh4UMBAhAUAACAIQGDAAwEDGBAIEIDAgACZAWwGIADCgwUMAABQACBAAAYIDgSIAMA1AgIBZAOg3WBAANwEDATg3TsAgQAHIAAIUNwUOIIJABoAGBDA+fPnBAIcKNBAQkAAOw==\" id=\"Inception V3_mixed10_1\" preserveAspectRatio=\"none\" width=\"133\" x=\"108.5\" y=\"30\"><title>Layer: mixed10 (output)\n",
       " Keras class = Concatenate\n",
       " trainable = True\n",
       " axis = 3</title></image><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"246.5\" y=\"132.0\">mixed10</text><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"246.5\" y=\"35\">2048</text><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"96.5\" y=\"225\">0</text><rect height=\"27\" style=\"fill:none;stroke:black;stroke-width:2\" width=\"202\" x=\"74.0\" y=\"1779\"/><image class=\"Inception V3_input_1\" height=\"25\" href=\"data:image/gif;base64,R0lGODdhZAABAIcAAP/9/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAZAABAAAIEgABCBxIsKDBgwgTKlzIsGHCgAA7\" id=\"Inception V3_input_1_2\" preserveAspectRatio=\"none\" width=\"200\" x=\"75.0\" y=\"1780\"><title>Layer: input_1 (input)\n",
       " Keras class = Input\n",
       " batch_shape = (None, None, None, 3)</title></image><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"280.0\" y=\"1794.5\">input_1</text><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"280.0\" y=\"1785\">3</text><text alignment-baseline=\"central\" font-family=\"monospace\" font-size=\"12\" text-anchor=\"start\" x=\"63.0\" y=\"1800\">0</text></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "SVG(network.build_svg(opts={\"svg_height\": 2000}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAEsCAYAAACG+vy+AAAKO0lEQVR4nO3d/6+edX3H8ev2dLMZ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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=200x300 at 0x7F358077BC18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAEsCAYAAACG+vy+AAAHSElEQVR4nO3df6jddR3H8XO6I4RA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\nIBAIBAKBQCAQCAQCgUAgEAgE/wJbzTOTlacxNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=200x300 at 0x7F358077BC18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAEsCAYAAACG+vy+AAAGu0lEQVR4nO3dz4vUdRzH8Zlm7eKu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\nyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=200x300 at 0x7F358077BC18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "for i in range(3):\n",
    "    network[\"mixed10\"].feature = i\n",
    "    display(network.propagate_to_image(\"mixed10\", img[0]).resize((200, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = network.propagate(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20, 2048)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from conx import shape\n",
    "shape(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv2d_1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.layers[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = network.propagate_to(\"conv2d_1\", img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 343, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 687, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Shape(network.dataset.inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss.\n",
    "loss = K.variable(0.)\n",
    "for layer_name in settings['features']:\n",
    "    # Add the L2 norm of the features of a layer to the loss.\n",
    "    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.'\n",
    "    coeff = settings['features'][layer_name]\n",
    "    x = layer_dict[layer_name].output\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    scaling = K.prod(K.cast(K.shape(x), 'float32'))\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss += coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling\n",
    "    else:\n",
    "        loss += coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling\n",
    "\n",
    "# Compute the gradients of the dream wrt the loss.\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "# Normalize gradients.\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# Set up function to retrieve the value\n",
    "# of the loss and gradients given an input image.\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:\n",
    "\n",
    "- Load the original image.\n",
    "- Define a number of processing scales (i.e. image shapes),\n",
    "    from smallest to largest.\n",
    "- Resize the original image to the smallest scale.\n",
    "- For every scale, starting with the smallest (i.e. current one):\n",
    "    - Run gradient ascent\n",
    "    - Upscale image to the next scale\n",
    "    - Reinject the detail that was lost at upscaling time\n",
    "- Stop when we are back to the original size.\n",
    "\n",
    "To obtain the detail lost during upscaling, we simply\n",
    "take the original image, shrink it down, upscale it,\n",
    "and compare the result to the (resized) original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with these hyperparameters will also allow you to achieve new effects\n",
    "step = 0.01  # Gradient ascent step size\n",
    "num_octave = 3  # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4  # Size ratio between scales\n",
    "iterations = 20  # Number of ascent steps per scale\n",
    "max_loss = 10.\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    original_shape = img.shape[2:]\n",
    "else:\n",
    "    original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model.predict(img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = network.propagate(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output1 == output2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conx import shape as Shape\n",
    "Shape(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    display(array2image(output1[[slice(None, None), slice(None, None), i]]).resize((200, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img,\n",
    "                          iterations=iterations,\n",
    "                          step=step,\n",
    "                          max_loss=max_loss)\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "\n",
    "save_img(img, fname=result_prefix + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"./monalisa-generated.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
