{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAPc0lEQVR4nO1dWY/cyJFOkkkWWXdV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\n/R+Z1dd5hPzx7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100 at 0x7F820D902A20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = PIL.Image.fromarray(x_train[0]).resize((100,100))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conx import *\n",
    "\n",
    "net = Network(\"CIRAR10\")\n",
    "net.add(Layer(\"input\", (32, 32, 3))) # depends on K.image_data_format(), right?\n",
    "net.add(Conv2DLayer(\"conv1\", 32, (3, 3), padding='same', activation='relu'))\n",
    "net.add(Conv2DLayer(\"conv2\", 32, (3, 3), activation='relu'))\n",
    "net.add(MaxPool2DLayer(\"pool1\", pool_size=(2, 2), dropout=0.25))\n",
    "net.add(Conv2DLayer(\"conv3\", 64, (3, 3), padding='same', activation='relu'))\n",
    "net.add(Conv2DLayer(\"conv4\", 64, (3, 3), activation='relu'))\n",
    "net.add(MaxPool2DLayer(\"pool2\", pool_size=(2, 2), dropout=0.25))\n",
    "net.add(FlattenLayer(\"flatten\"))\n",
    "net.add(Layer(\"hidden1\", 512, activation='relu', vshape=(16, 32), dropout=0.5))\n",
    "net.add(Layer(\"output\", num_classes, activation='softmax'))\n",
    "net.connect()\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model = net.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['base/js/namespace'], function(Jupyter) {\n",
       "    Jupyter.notebook.kernel.comm_manager.register_target('conx_svg_control', function(comm, msg) {\n",
       "        comm.on_msg(function(msg) {\n",
       "            var data = msg[\"content\"][\"data\"];\n",
       "            var images = document.getElementsByClassName(data[\"class\"]);\n",
       "            for (var i = 0; i < images.length; i++) {\n",
       "                images[i].setAttributeNS(null, \"href\", data[\"href\"]);\n",
       "            }\n",
       "        });\n",
       "    });\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605bd299dbe44a789626ac67bb239f92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10481584,\n",
       " 0.10282389,\n",
       " 0.093064144,\n",
       " 0.095247671,\n",
       " 0.1024968,\n",
       " 0.097768381,\n",
       " 0.10347322,\n",
       " 0.090928726,\n",
       " 0.10609886,\n",
       " 0.10328246]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.propagate(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Summary:\n",
      "   count  : 50000\n",
      "   shape  : (32, 32, 3)\n",
      "   range  : (0.0, 1.0)\n",
      "Target Summary:\n",
      "   count  : 50000\n",
      "   shape  : (10,)\n",
      "   range  : (0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "net.set_dataset_direct(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = net.model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 3, 32), (32,)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolution filter is (3,3)\n",
    "* Image is (32, 32, 3) (because of K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][0,0].shape # image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][:,:,0,0].shape # just the first two dimensions, the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0448245 , -0.05102114,  0.02703777],\n",
       "       [ 0.01533299,  0.03206208, -0.1197199 ],\n",
       "       [ 0.05765197, -0.11089787,  0.13460267]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 3 * 3 * 32 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1068515,\n",
       " 0.10157881,\n",
       " 0.09088359,\n",
       " 0.095104292,\n",
       " 0.10231216,\n",
       " 0.096545465,\n",
       " 0.1019525,\n",
       " 0.094712354,\n",
       " 0.10622536,\n",
       " 0.10383396]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.propagate(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0301b86f467b4078855b366a1a5cb5ed": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_2c612b2259264b39ba109aa235189264",
        "IPY_MODEL_4f7f600c0e524520af58b753bf9266e1"
       ],
       "layout": "IPY_MODEL_66a698cbc6d74b5fb0b9b84b6d70d1d5"
      }
     },
     "0778836d64374b0b8785d81e3e20d75a": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "width": "100%"
      }
     },
     "09d8f3f81abf42809d7b9423b835a8c9": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "0cefb929d282465cb8d89d0c95989889": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "1dc1ebc69cee49a182d5d02f97b53deb": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_6a7c5e0ad95c484eb0e4fda1c572720d"
       ],
       "layout": "IPY_MODEL_a7bc298204e04f63b3257b79c8459f98"
      }
     },
     "26302e742d6b480fad19c7cb433284cc": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_3f007b9aae2d4fd2b2ea54c1c3cc1914",
        "IPY_MODEL_aafec1e51eec4734a45af27c9466dd7f",
        "IPY_MODEL_d99ff534d120431e8b3ed6071d025ee3",
        "IPY_MODEL_b636c7ddd51f46599596941f54df0472",
        "IPY_MODEL_5d232a6e9a164368ad2264d73c92af5f"
       ],
       "layout": "IPY_MODEL_915a37140ac744659a79f7d16a86720e"
      }
     },
     "29d4c875403844bd91c8dfc54f2d3ec9": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "2c612b2259264b39ba109aa235189264": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_6d5024c9b24e4e5baf4cf3392e26818d",
       "value": "\n        <svg id='CIRAR10' xmlns='http://www.w3.org/2000/svg' width=\"350\" height=\"1875\" image-rendering=\"pixelated\">\n    <defs>\n        <marker id=\"arrow\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\" markerUnits=\"strokeWidth\">\n          <path d=\"M0,0 L0,6 L9,3 z\" fill=\"blue\" />\n        </marker>\n    </defs>\n<rect x=\"74.0\" y=\"29\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_output_1\" class=\"CIRAR10_output\" x=\"75.0\" y=\"30\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhCgABAIcAALAXKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAACgABAAAIBwABCBxIMCAAOw==\"><title>Layer: output (output)\n shape = (10,)\n Keras class = Dense\n activation = softmax</title></image><text x=\"280.0\" y=\"44.5\" font-family=\"monospace\" font-size=\"12\">output</text><rect x=\"165.0\" y=\"57\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from hidden1 to output\n output/kernel has shape (512, 10)\n output/bias has shape (10,)</title></rect><line x1=\"175.0\" y1=\"84\" x2=\"175.0\" y2=\"57\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from hidden1 to output\n output/kernel has shape (512, 10)\n output/bias has shape (10,)</title></line><rect x=\"74.0\" y=\"84\" width=\"202\" height=\"102\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_hidden1_2\" class=\"CIRAR10_hidden1\" x=\"75.0\" y=\"85\" height=\"100\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAQAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAIAAQAEAIJwABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjxADAgA7\"><title>Layer: hidden1 (hidden)\n shape = (512,)\n dropout = 0.5\n Keras class = Dense\n activation = relu</title></image><text x=\"280.0\" y=\"137.0\" font-family=\"monospace\" font-size=\"12\">hidden1</text><rect x=\"165.0\" y=\"187\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from flatten to hidden1\n hidden1/kernel has shape (2304, 512)\n hidden1/bias has shape (512,)</title></rect><line x1=\"175.0\" y1=\"214\" x2=\"175.0\" y2=\"187\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from flatten to hidden1\n hidden1/kernel has shape (2304, 512)\n hidden1/bias has shape (512,)</title></line><rect x=\"74.0\" y=\"214\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_flatten_3\" class=\"CIRAR10_flatten\" x=\"75.0\" y=\"215\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhAAkBAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAAAkBAAAITwABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh2qMSAAOw==\"><title>Layer: flatten (hidden)\n Keras class = Flatten</title></image><text x=\"280.0\" y=\"229.5\" font-family=\"monospace\" font-size=\"12\">flatten</text><rect x=\"165.0\" y=\"242\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from pool2 to flatten</title></rect><line x1=\"175.0\" y1=\"269\" x2=\"175.0\" y2=\"242\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from pool2 to flatten</title></line><rect x=\"74.0\" y=\"269\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_pool2_4\" class=\"CIRAR10_pool2\" x=\"75.0\" y=\"270\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhBgAGAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAABgAGAAAIDAABCBxIsKDBgwgDAgA7\"><title>Layer: pool2 (hidden)\n dropout = 0.25\n Keras class = MaxPooling2D\n pool_size = (2, 2)</title></image><text x=\"280.0\" y=\"372.0\" font-family=\"monospace\" font-size=\"12\">pool2</text><rect x=\"165.0\" y=\"472\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv4 to pool2</title></rect><line x1=\"175.0\" y1=\"499\" x2=\"175.0\" y2=\"472\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv4 to pool2</title></line><rect x=\"74.0\" y=\"499\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv4_5\" class=\"CIRAR10_conv4\" x=\"75.0\" y=\"500\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDQANAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAADQANAAAIFwABCBxIsKDBgwgTKlzIsKHDhxAjQgwIADs=\"><title>Layer: conv4 (hidden)\n Keras class = Conv2D\n activation = relu</title></image><text x=\"280.0\" y=\"602.0\" font-family=\"monospace\" font-size=\"12\">conv4</text><rect x=\"165.0\" y=\"702\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv3 to conv4\n conv4/kernel has shape (3, 3, 64, 64)\n conv4/bias has shape (64,)</title></rect><line x1=\"175.0\" y1=\"729\" x2=\"175.0\" y2=\"702\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv3 to conv4\n conv4/kernel has shape (3, 3, 64, 64)\n conv4/bias has shape (64,)</title></line><rect x=\"74.0\" y=\"729\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv3_6\" class=\"CIRAR10_conv3\" x=\"75.0\" y=\"730\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDwAPAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAADwAPAAAIGgABCBxIsKDBgwgTKlzIsKHDhxAjSpxI8WFAADs=\"><title>Layer: conv3 (hidden)\n Keras class = Conv2D\n padding = same\n activation = relu</title></image><text x=\"280.0\" y=\"832.0\" font-family=\"monospace\" font-size=\"12\">conv3</text><rect x=\"165.0\" y=\"932\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from pool1 to conv3\n conv3/kernel has shape (3, 3, 32, 64)\n conv3/bias has shape (64,)</title></rect><line x1=\"175.0\" y1=\"959\" x2=\"175.0\" y2=\"932\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from pool1 to conv3\n conv3/kernel has shape (3, 3, 32, 64)\n conv3/bias has shape (64,)</title></line><rect x=\"74.0\" y=\"959\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_pool1_7\" class=\"CIRAR10_pool1\" x=\"75.0\" y=\"960\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDwAPAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAADwAPAAAIGgABCBxIsKDBgwgTKlzIsKHDhxAjSpxI8WFAADs=\"><title>Layer: pool1 (hidden)\n dropout = 0.25\n Keras class = MaxPooling2D\n pool_size = (2, 2)</title></image><text x=\"280.0\" y=\"1062.0\" font-family=\"monospace\" font-size=\"12\">pool1</text><rect x=\"165.0\" y=\"1162\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv2 to pool1</title></rect><line x1=\"175.0\" y1=\"1189\" x2=\"175.0\" y2=\"1162\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv2 to pool1</title></line><rect x=\"74.0\" y=\"1189\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv2_8\" class=\"CIRAR10_conv2\" x=\"75.0\" y=\"1190\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhHgAeAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAHgAeAEAIMgABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTngwIADs=\"><title>Layer: conv2 (hidden)\n Keras class = Conv2D\n activation = relu</title></image><text x=\"280.0\" y=\"1292.0\" font-family=\"monospace\" font-size=\"12\">conv2</text><rect x=\"165.0\" y=\"1392\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv1 to conv2\n conv2/kernel has shape (3, 3, 32, 32)\n conv2/bias has shape (32,)</title></rect><line x1=\"175.0\" y1=\"1419\" x2=\"175.0\" y2=\"1392\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv1 to conv2\n conv2/kernel has shape (3, 3, 32, 32)\n conv2/bias has shape (32,)</title></line><rect x=\"74.0\" y=\"1419\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv1_9\" class=\"CIRAR10_conv1\" x=\"75.0\" y=\"1420\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAgAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAIAAgAEAINQABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJUmRAADs=\"><title>Layer: conv1 (hidden)\n Keras class = Conv2D\n padding = same\n activation = relu</title></image><text x=\"280.0\" y=\"1522.0\" font-family=\"monospace\" font-size=\"12\">conv1</text><rect x=\"165.0\" y=\"1622\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from input to conv1\n conv1/kernel has shape (3, 3, 3, 32)\n conv1/bias has shape (32,)</title></rect><line x1=\"175.0\" y1=\"1649\" x2=\"175.0\" y2=\"1622\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from input to conv1\n conv1/kernel has shape (3, 3, 3, 32)\n conv1/bias has shape (32,)</title></line><rect x=\"74.0\" y=\"1649\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_input_10\" class=\"CIRAR10_input\" x=\"75.0\" y=\"1650\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAgAIcAAP7+/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwAAAAAIAAgAEAINQABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJUmRAADs=\"><title>Layer: input (input)\n shape = (32, 32, 3)\n Keras class = Input</title></image><text x=\"280.0\" y=\"1752.0\" font-family=\"monospace\" font-size=\"12\">input</text></svg>"
      }
     },
     "30fbf21a08164fa6b117143fa82d25a2": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "3f007b9aae2d4fd2b2ea54c1c3cc1914": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "fast-backward",
       "layout": "IPY_MODEL_af6eb10f856749f09530c3e9764e23e2",
       "style": "IPY_MODEL_9519b6e2bd174976849f2d2e60a6672e"
      }
     },
     "4f7f600c0e524520af58b753bf9266e1": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_7abbd7602808409a8d3f962e35cdefc0",
        "IPY_MODEL_8c05746e41fa4747950c93bdaa77e013",
        "IPY_MODEL_26302e742d6b480fad19c7cb433284cc"
       ],
       "layout": "IPY_MODEL_30fbf21a08164fa6b117143fa82d25a2"
      }
     },
     "537fba4368ad4ce8af7fb1f5c1259e35": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "width": "100%"
      }
     },
     "5d232a6e9a164368ad2264d73c92af5f": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "fast-forward",
       "layout": "IPY_MODEL_29d4c875403844bd91c8dfc54f2d3ec9",
       "style": "IPY_MODEL_e79bc13de2cb405aae8dcf4fd46ba434"
      }
     },
     "605bd299dbe44a789626ac67bb239f92": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "TabModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_titles": {
        "0": "Network",
        "1": "Graphs",
        "2": "Analysis",
        "3": "Camera",
        "4": "Help"
       },
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_0301b86f467b4078855b366a1a5cb5ed",
        "IPY_MODEL_883efcc6cc3645329027698a4c3de358",
        "IPY_MODEL_f589ec7709cd424fa065572125f3ffc0",
        "IPY_MODEL_1dc1ebc69cee49a182d5d02f97b53deb",
        "IPY_MODEL_bd971b936e1c4c9e869d33b4036ba2d8"
       ],
       "layout": "IPY_MODEL_dcb29f811e0d4a3392e91ca20b07bc44"
      }
     },
     "66a698cbc6d74b5fb0b9b84b6d70d1d5": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "width": "100%"
      }
     },
     "6a7c5e0ad95c484eb0e4fda1c572720d": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "description": "Turn on webcamera",
       "layout": "IPY_MODEL_8fccc328726a4f739390dacf496baeff",
       "style": "IPY_MODEL_84826c2eec5f4eeba3c5c3c85727b3d9"
      }
     },
     "6d5024c9b24e4e5baf4cf3392e26818d": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "justify_content": "center",
       "overflow_x": "auto",
       "width": "100%"
      }
     },
     "7abbd7602808409a8d3f962e35cdefc0": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "SelectModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_options_labels": [
        "Test",
        "Train"
       ],
       "_view_module_version": "~2.1.4",
       "description": "Dataset:",
       "layout": "IPY_MODEL_cb996b8e96ad4372967c98968c945733",
       "value": "Train"
      }
     },
     "84826c2eec5f4eeba3c5c3c85727b3d9": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "883efcc6cc3645329027698a4c3de358": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_537fba4368ad4ce8af7fb1f5c1259e35"
      }
     },
     "8c05746e41fa4747950c93bdaa77e013": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "IntSliderModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "continuous_update": false,
       "description": "Dataset index",
       "layout": "IPY_MODEL_09d8f3f81abf42809d7b9423b835a8c9",
       "max": 0,
       "style": "IPY_MODEL_91e276a0b23845b3a18bf31827293a8b"
      }
     },
     "8c31dc78e3de428e8f035de39951ff97": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "8fccc328726a4f739390dacf496baeff": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "915a37140ac744659a79f7d16a86720e": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "91e276a0b23845b3a18bf31827293a8b": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "9519b6e2bd174976849f2d2e60a6672e": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "9e98c9a9e2d0480e9b5ad41eb2d44c24": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "a523c872845147c89a5ef4fa173206ad": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "width": "100%"
      }
     },
     "a7bc298204e04f63b3257b79c8459f98": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "550px",
       "width": "100%"
      }
     },
     "aafec1e51eec4734a45af27c9466dd7f": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "backward",
       "layout": "IPY_MODEL_0cefb929d282465cb8d89d0c95989889",
       "style": "IPY_MODEL_8c31dc78e3de428e8f035de39951ff97"
      }
     },
     "af6eb10f856749f09530c3e9764e23e2": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "b636c7ddd51f46599596941f54df0472": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "forward",
       "layout": "IPY_MODEL_9e98c9a9e2d0480e9b5ad41eb2d44c24",
       "style": "IPY_MODEL_cbcf7c8fb8ef403a814e2283c8761ee1"
      }
     },
     "bd971b936e1c4c9e869d33b4036ba2d8": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_a523c872845147c89a5ef4fa173206ad",
       "value": "<iframe style=\"width: 960px\" src=\"https://conx.readthedocs.io\" width=\"100%\" height=\"550px\"></frame>"
      }
     },
     "c5d7a16049d64afe885919b56fd37a87": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "cb996b8e96ad4372967c98968c945733": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "cbcf7c8fb8ef403a814e2283c8761ee1": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "d99ff534d120431e8b3ed6071d025ee3": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "description": "Train",
       "layout": "IPY_MODEL_c5d7a16049d64afe885919b56fd37a87",
       "style": "IPY_MODEL_ede4f2ccd4354e4ba59c9aa6b558abb5"
      }
     },
     "dcb29f811e0d4a3392e91ca20b07bc44": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "e79bc13de2cb405aae8dcf4fd46ba434": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "ede4f2ccd4354e4ba59c9aa6b558abb5": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "f589ec7709cd424fa065572125f3ffc0": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_0778836d64374b0b8785d81e3e20d75a"
      }
     }
    },
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
