{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 32, 32, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAPc0lEQVR4nO1dWY/cyJFOkkkWWXdV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\n/R+Z1dd5hPzx7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x100 at 0x7F96873DFA90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = PIL.Image.fromarray(x_train[0]).resize((100,100))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from conx import *\n",
    "\n",
    "net = Network(\"CIRAR10\")\n",
    "net.add(Layer(\"input\", (32, 32, 3))) # depends on K.image_data_format(), right?\n",
    "net.add(Conv2DLayer(\"conv1\", 32, (3, 3), padding='same', activation='relu'))\n",
    "net.add(Conv2DLayer(\"conv2\", 32, (3, 3), activation='relu'))\n",
    "net.add(MaxPool2DLayer(\"pool1\", pool_size=(2, 2), dropout=0.25))\n",
    "net.add(Conv2DLayer(\"conv3\", 64, (3, 3), padding='same', activation='relu'))\n",
    "net.add(Conv2DLayer(\"conv4\", 64, (3, 3), activation='relu'))\n",
    "net.add(MaxPool2DLayer(\"pool2\", pool_size=(2, 2), dropout=0.25))\n",
    "net.add(FlattenLayer(\"flatten\"))\n",
    "net.add(Layer(\"hidden1\", 512, activation='relu', vshape=(16, 32), dropout=0.5))\n",
    "net.add(Layer(\"output\", num_classes, activation='softmax'))\n",
    "net.connect()\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "net.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model = net.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['base/js/namespace'], function(Jupyter) {\n",
       "    Jupyter.notebook.kernel.comm_manager.register_target('conx_svg_control', function(comm, msg) {\n",
       "        comm.on_msg(function(msg) {\n",
       "            var data = msg[\"content\"][\"data\"];\n",
       "            var images = document.getElementsByClassName(data[\"class\"]);\n",
       "            for (var i = 0; i < images.length; i++) {\n",
       "                images[i].setAttributeNS(null, \"href\", data[\"href\"]);\n",
       "            }\n",
       "        });\n",
       "    });\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64edfc12ce3400c9737b9288d6b4efa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1005762,\n",
       " 0.094180107,\n",
       " 0.10122274,\n",
       " 0.10924178,\n",
       " 0.097026072,\n",
       " 0.10193674,\n",
       " 0.097991653,\n",
       " 0.09543433,\n",
       " 0.1021902,\n",
       " 0.10020017]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.propagate(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Summary:\n",
      "   length  : 50000\n",
      "   training: 50000\n",
      "   testing : 0\n",
      "   shape  : (32, 32, 3)\n",
      "   range  : (0.0, 1.0)\n",
      "Target Summary:\n",
      "   length  : 50000\n",
      "   training: 50000\n",
      "   testing : 0\n",
      "   shape  : (10,)\n",
      "   range  : (0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "net.set_dataset_direct(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net.model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3, 3, 32), (32,)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_last'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolution filter is (3,3)\n",
    "* Image is (32, 32, 3) (because of K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][0,0].shape # image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][:,:,0,0].shape # just the first two dimensions, the filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01075616,  0.01086534, -0.0041749 ],\n",
       "       [-0.10892708, -0.10641845, -0.04017058],\n",
       "       [-0.11515775,  0.01121084, -0.09700422]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][:,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 3 * 3 * 32 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10593142,\n",
       " 0.090718798,\n",
       " 0.11996857,\n",
       " 0.093882129,\n",
       " 0.091921531,\n",
       " 0.099259503,\n",
       " 0.10394023,\n",
       " 0.093262888,\n",
       " 0.10208549,\n",
       " 0.099029452]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.propagate(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 455s - loss: 1.8629 - acc: 0.3134 - val_loss: 1.5768 - val_acc: 0.4368\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 471s - loss: 1.5649 - acc: 0.4293 - val_loss: 1.3569 - val_acc: 0.5179\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 470s - loss: 1.4413 - acc: 0.4797 - val_loss: 1.3091 - val_acc: 0.5330\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 468s - loss: 1.3592 - acc: 0.5117 - val_loss: 1.1923 - val_acc: 0.5762\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 446s - loss: 1.2914 - acc: 0.5387 - val_loss: 1.1345 - val_acc: 0.6012\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 432s - loss: 1.2267 - acc: 0.5622 - val_loss: 1.0649 - val_acc: 0.6279\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 435s - loss: 1.1752 - acc: 0.5843 - val_loss: 1.0042 - val_acc: 0.6471\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 445s - loss: 1.1317 - acc: 0.6001 - val_loss: 0.9749 - val_acc: 0.6543\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 434s - loss: 1.0913 - acc: 0.6144 - val_loss: 0.9299 - val_acc: 0.6777\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 424s - loss: 1.0559 - acc: 0.6285 - val_loss: 0.9070 - val_acc: 0.6812\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 420s - loss: 1.0240 - acc: 0.6368 - val_loss: 0.9548 - val_acc: 0.6617\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 425s - loss: 0.9994 - acc: 0.6505 - val_loss: 0.8736 - val_acc: 0.6965\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 425s - loss: 0.9742 - acc: 0.6600 - val_loss: 0.8313 - val_acc: 0.7110\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 423s - loss: 0.9609 - acc: 0.6628 - val_loss: 0.8184 - val_acc: 0.7149\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 425s - loss: 0.9431 - acc: 0.6706 - val_loss: 0.7993 - val_acc: 0.7265\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 422s - loss: 0.9289 - acc: 0.6762 - val_loss: 0.7962 - val_acc: 0.7219\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 425s - loss: 0.9091 - acc: 0.6812 - val_loss: 0.7711 - val_acc: 0.7346\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 425s - loss: 0.8990 - acc: 0.6866 - val_loss: 0.7542 - val_acc: 0.7453\n",
      "Epoch 19/200\n",
      " 569/1562 [=========>....................] - ETA: 294s - loss: 0.8812 - acc: 0.6917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d7688794956b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                         validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                      batch_size=batch_size),\n",
    "                                      steps=x_test.shape[0] // batch_size)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09b9dabe61b3418e9d7ae0bfc065b199": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_a21621f58d3a4ad4b8d7eaa45d8824d8",
        "IPY_MODEL_925091b548e94767a11cf5467a867f03",
        "IPY_MODEL_ff4e2d14a8894bf6aea44b4fff91fece"
       ],
       "layout": "IPY_MODEL_2010920d85944839be39f4abd9266232"
      }
     },
     "0d3173d618e5425ebf1b7b8fca0f9807": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "backward",
       "layout": "IPY_MODEL_14aaebb00a634c869560ebd44a613c74",
       "style": "IPY_MODEL_57e8d80ec06846dbb91c3ca7d8e948e2"
      }
     },
     "14aaebb00a634c869560ebd44a613c74": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "1e2262e91dfc4511aad4d897a00b4ed8": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "layout": "IPY_MODEL_82f135b20d0841318f5d91936ba2cf32",
       "value": "\n        <svg id='CIRAR10' xmlns='http://www.w3.org/2000/svg' width=\"350\" height=\"1945\" image-rendering=\"pixelated\">\n    <defs>\n        <marker id=\"arrow\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\" markerUnits=\"strokeWidth\">\n          <path d=\"M0,0 L0,6 L9,3 z\" fill=\"blue\" />\n        </marker>\n    </defs>\n<rect x=\"74.0\" y=\"24\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_output_targets_1\" class=\"CIRAR10_output_targets\" x=\"75.0\" y=\"25\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhCgABAIAAABkZGQAAACwAAAAACgABAAAIBwABCBxIMCAAOw==\"><title>Layer: output (output)\n shape = (10,)\n Keras class = Dense\n activation = softmax</title></image><text x=\"280.0\" y=\"39.5\" font-family=\"monospace\" font-size=\"12\">targets</text><rect x=\"74.0\" y=\"59\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_output_errors_1\" class=\"CIRAR10_output_errors\" x=\"75.0\" y=\"60\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhCgABAIAAABkZGQAAACwAAAAACgABAAAIBwABCBxIMCAAOw==\"><title>Layer: output (output)\n shape = (10,)\n Keras class = Dense\n activation = softmax</title></image><text x=\"280.0\" y=\"74.5\" font-family=\"monospace\" font-size=\"12\">errors</text><rect x=\"74.0\" y=\"99\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_output_1\" class=\"CIRAR10_output\" x=\"75.0\" y=\"100\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhCgABAIAAABkZGQAAACwAAAAACgABAAAIBwABCBxIMCAAOw==\"><title>Layer: output (output)\n shape = (10,)\n Keras class = Dense\n activation = softmax</title></image><text x=\"280.0\" y=\"114.5\" font-family=\"monospace\" font-size=\"12\">output</text><rect x=\"165.0\" y=\"127\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from hidden1 to output\n output/kernel has shape (512, 10)\n output/bias has shape (10,)</title></rect><line x1=\"175.0\" y1=\"154\" x2=\"175.0\" y2=\"127\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from hidden1 to output\n output/kernel has shape (512, 10)\n output/bias has shape (10,)</title></line><rect x=\"74.0\" y=\"154\" width=\"202\" height=\"102\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_hidden1_2\" class=\"CIRAR10_hidden1\" x=\"75.0\" y=\"155\" height=\"100\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAQAIAAAH9/fwAAACwAAAAAIAAQAEAIJwABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjxADAgA7\"><title>Layer: hidden1 (hidden)\n shape = (512,)\n dropout = 0.5\n Keras class = Dense\n activation = relu</title></image><text x=\"280.0\" y=\"207.0\" font-family=\"monospace\" font-size=\"12\">hidden1</text><rect x=\"165.0\" y=\"257\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from flatten to hidden1\n hidden1/kernel has shape (2304, 512)\n hidden1/bias has shape (512,)</title></rect><line x1=\"175.0\" y1=\"284\" x2=\"175.0\" y2=\"257\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from flatten to hidden1\n hidden1/kernel has shape (2304, 512)\n hidden1/bias has shape (512,)</title></line><rect x=\"74.0\" y=\"284\" width=\"202\" height=\"27\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_flatten_3\" class=\"CIRAR10_flatten\" x=\"75.0\" y=\"285\" height=\"25\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhAAkBAIAAAH9/fwAAACwAAAAAAAkBAAAITwABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh2qMSAAOw==\"><title>Layer: flatten (hidden)\n Keras class = Flatten</title></image><text x=\"280.0\" y=\"299.5\" font-family=\"monospace\" font-size=\"12\">flatten</text><rect x=\"165.0\" y=\"312\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from pool2 to flatten</title></rect><line x1=\"175.0\" y1=\"339\" x2=\"175.0\" y2=\"312\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from pool2 to flatten</title></line><rect x=\"74.0\" y=\"339\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_pool2_4\" class=\"CIRAR10_pool2\" x=\"75.0\" y=\"340\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhBgAGAIAAAH9/fwAAACwAAAAABgAGAAAIDAABCBxIsKDBgwgDAgA7\"><title>Layer: pool2 (hidden)\n dropout = 0.25\n Keras class = MaxPooling2D\n pool_size = (2, 2)</title></image><text x=\"280.0\" y=\"442.0\" font-family=\"monospace\" font-size=\"12\">pool2</text><rect x=\"165.0\" y=\"542\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv4 to pool2</title></rect><line x1=\"175.0\" y1=\"569\" x2=\"175.0\" y2=\"542\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv4 to pool2</title></line><rect x=\"74.0\" y=\"569\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv4_5\" class=\"CIRAR10_conv4\" x=\"75.0\" y=\"570\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDQANAIAAAH9/fwAAACwAAAAADQANAAAIFwABCBxIsKDBgwgTKlzIsKHDhxAjQgwIADs=\"><title>Layer: conv4 (hidden)\n Keras class = Conv2D\n activation = relu</title></image><text x=\"280.0\" y=\"672.0\" font-family=\"monospace\" font-size=\"12\">conv4</text><rect x=\"165.0\" y=\"772\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv3 to conv4\n conv4/kernel has shape (3, 3, 64, 64)\n conv4/bias has shape (64,)</title></rect><line x1=\"175.0\" y1=\"799\" x2=\"175.0\" y2=\"772\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv3 to conv4\n conv4/kernel has shape (3, 3, 64, 64)\n conv4/bias has shape (64,)</title></line><rect x=\"74.0\" y=\"799\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv3_6\" class=\"CIRAR10_conv3\" x=\"75.0\" y=\"800\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDwAPAIAAAH9/fwAAACwAAAAADwAPAAAIGgABCBxIsKDBgwgTKlzIsKHDhxAjSpxI8WFAADs=\"><title>Layer: conv3 (hidden)\n Keras class = Conv2D\n padding = same\n activation = relu</title></image><text x=\"280.0\" y=\"902.0\" font-family=\"monospace\" font-size=\"12\">conv3</text><rect x=\"165.0\" y=\"1002\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from pool1 to conv3\n conv3/kernel has shape (3, 3, 32, 64)\n conv3/bias has shape (64,)</title></rect><line x1=\"175.0\" y1=\"1029\" x2=\"175.0\" y2=\"1002\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from pool1 to conv3\n conv3/kernel has shape (3, 3, 32, 64)\n conv3/bias has shape (64,)</title></line><rect x=\"74.0\" y=\"1029\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_pool1_7\" class=\"CIRAR10_pool1\" x=\"75.0\" y=\"1030\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhDwAPAIAAAH9/fwAAACwAAAAADwAPAAAIGgABCBxIsKDBgwgTKlzIsKHDhxAjSpxI8WFAADs=\"><title>Layer: pool1 (hidden)\n dropout = 0.25\n Keras class = MaxPooling2D\n pool_size = (2, 2)</title></image><text x=\"280.0\" y=\"1132.0\" font-family=\"monospace\" font-size=\"12\">pool1</text><rect x=\"165.0\" y=\"1232\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv2 to pool1</title></rect><line x1=\"175.0\" y1=\"1259\" x2=\"175.0\" y2=\"1232\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv2 to pool1</title></line><rect x=\"74.0\" y=\"1259\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv2_8\" class=\"CIRAR10_conv2\" x=\"75.0\" y=\"1260\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhHgAeAIAAAH9/fwAAACwAAAAAHgAeAEAIMgABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTngwIADs=\"><title>Layer: conv2 (hidden)\n Keras class = Conv2D\n activation = relu</title></image><text x=\"280.0\" y=\"1362.0\" font-family=\"monospace\" font-size=\"12\">conv2</text><rect x=\"165.0\" y=\"1462\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from conv1 to conv2\n conv2/kernel has shape (3, 3, 32, 32)\n conv2/bias has shape (32,)</title></rect><line x1=\"175.0\" y1=\"1489\" x2=\"175.0\" y2=\"1462\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from conv1 to conv2\n conv2/kernel has shape (3, 3, 32, 32)\n conv2/bias has shape (32,)</title></line><rect x=\"74.0\" y=\"1489\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_conv1_9\" class=\"CIRAR10_conv1\" x=\"75.0\" y=\"1490\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAgAIAAAH9/fwAAACwAAAAAIAAgAEAINQABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJUmRAADs=\"><title>Layer: conv1 (hidden)\n Keras class = Conv2D\n padding = same\n activation = relu</title></image><text x=\"280.0\" y=\"1592.0\" font-family=\"monospace\" font-size=\"12\">conv1</text><rect x=\"165.0\" y=\"1692\" width=\"20.0\" height=\"27\" style=\"fill:white;stroke:none\"><title>Weights from input to conv1\n conv1/kernel has shape (3, 3, 3, 32)\n conv1/bias has shape (32,)</title></rect><line x1=\"175.0\" y1=\"1719\" x2=\"175.0\" y2=\"1692\" stroke=\"blue\" stroke-width=\"2\" marker-end=\"url(#arrow)\"><title>Weights from input to conv1\n conv1/kernel has shape (3, 3, 3, 32)\n conv1/bias has shape (32,)</title></line><rect x=\"74.0\" y=\"1719\" width=\"202\" height=\"202\" style=\"fill:none;stroke:blue;stroke-width:2\"/><image id=\"CIRAR10_input_10\" class=\"CIRAR10_input\" x=\"75.0\" y=\"1720\" height=\"200\" width=\"200\" preserveAspectRatio=\"none\" href=\"data:image/gif;base64,R0lGODdhIAAgAIAAAH9/fwAAACwAAAAAIAAgAEAINQABCBxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJUmRAADs=\"><title>Layer: input (input)\n shape = (32, 32, 3)\n Keras class = Input</title></image><text x=\"280.0\" y=\"1822.0\" font-family=\"monospace\" font-size=\"12\">input</text></svg>"
      }
     },
     "2010920d85944839be39f4abd9266232": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "23ae8acf4aac4d5caf24352918d6fea7": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "4009fcf1e88644b7bfde189dfb39aa28": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "46396100509243c999d0837d9f8a4be2": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "57e8d80ec06846dbb91c3ca7d8e948e2": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "82f135b20d0841318f5d91936ba2cf32": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "height": "100%",
       "justify_content": "center",
       "max_height": "550px",
       "overflow_x": "auto",
       "width": "100%"
      }
     },
     "8bffa92f16354a058565545ccb9239d9": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "fast-backward",
       "layout": "IPY_MODEL_46396100509243c999d0837d9f8a4be2",
       "style": "IPY_MODEL_4009fcf1e88644b7bfde189dfb39aa28"
      }
     },
     "8dd81dc02a394afab0961348ddb76373": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "925091b548e94767a11cf5467a867f03": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "IntSliderModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "continuous_update": false,
       "description": "Dataset index",
       "layout": "IPY_MODEL_d70b82ddf26246b8baf14d591dcf0a8b",
       "max": 0,
       "style": "IPY_MODEL_c44223bcb644496c86eefe8abcc53ca7"
      }
     },
     "927555989aa545c3bc7a4844d571eb2b": {
      "model_module_version": "^2.1.4",
      "state": {
       "value": "<table style=\"width:100%\">\n                <thead>\n                    <tr>\n                        <th colspan=\"2\">\n                        Could not create model:\n                        </th>\n                    </tr>\n                </thead>\n                <tbody>\n                    <tr>\n                        <td>Model name</td>\n                        <td> CameraModel </td>\n                    </tr>\n                    <tr>\n                        <td>Model module</td>\n                        <td> camera </td>\n                    </tr>\n                    <tr>\n                        <td>Model module version</td>\n                        <td> * </td>\n                    </tr>\n                <tbody>\n                <tfoot>\n                    <tr>\n                        <th colspan=\"2\">\n                        Script error for \"camera\"\nhttp://requirejs.org/docs/errors.html#scripterror\n                        </th>\n                    </tr>\n                </tfoot>\n                </table>"
      }
     },
     "a1488b7d5ee14e11a9b6794413f3a240": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "a21621f58d3a4ad4b8d7eaa45d8824d8": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "SelectModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_options_labels": [
        "Test",
        "Train"
       ],
       "_view_module_version": "~2.1.4",
       "description": "Dataset:",
       "layout": "IPY_MODEL_8dd81dc02a394afab0961348ddb76373",
       "value": "Train"
      }
     },
     "a7e9d752682341e296016099abdd4401": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "b3ff4ad056f548489056f70819dd92bb": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "b6f58ea042864193ae06b67bc37db70b": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "bafcfeef05db4f1ba97ac3337656220a": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "forward",
       "layout": "IPY_MODEL_bc5b47a4584746fa98e239c9e09af32c",
       "style": "IPY_MODEL_a7e9d752682341e296016099abdd4401"
      }
     },
     "bc5b47a4584746fa98e239c9e09af32c": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "c44223bcb644496c86eefe8abcc53ca7": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "c6c65f5005c84768b95af19d4ea1b9e1": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "d05e4d66e30342c48fa151094a1d164e": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "icon": "fast-forward",
       "layout": "IPY_MODEL_b6f58ea042864193ae06b67bc37db70b",
       "style": "IPY_MODEL_c6c65f5005c84768b95af19d4ea1b9e1"
      }
     },
     "d70b82ddf26246b8baf14d591dcf0a8b": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "width": "100%"
      }
     },
     "d765eca970174751be3b1706c852304b": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "description": "Train",
       "layout": "IPY_MODEL_23ae8acf4aac4d5caf24352918d6fea7",
       "style": "IPY_MODEL_f32c8dbc677f455e9bae63f927e390d3"
      }
     },
     "e00107e1266f4f33b4ee5871395507cc": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "LayoutModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "e64edfc12ce3400c9737b9288d6b4efa": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "VBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_1e2262e91dfc4511aad4d897a00b4ed8",
        "IPY_MODEL_09b9dabe61b3418e9d7ae0bfc065b199"
       ],
       "layout": "IPY_MODEL_a1488b7d5ee14e11a9b6794413f3a240"
      }
     },
     "f32c8dbc677f455e9bae63f927e390d3": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "ButtonStyleModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4"
      }
     },
     "ff4e2d14a8894bf6aea44b4fff91fece": {
      "model_module": "jupyter-js-widgets",
      "model_module_version": "~2.1.4",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "~2.1.4",
       "_view_module_version": "~2.1.4",
       "children": [
        "IPY_MODEL_8bffa92f16354a058565545ccb9239d9",
        "IPY_MODEL_0d3173d618e5425ebf1b7b8fca0f9807",
        "IPY_MODEL_d765eca970174751be3b1706c852304b",
        "IPY_MODEL_bafcfeef05db4f1ba97ac3337656220a",
        "IPY_MODEL_d05e4d66e30342c48fa151094a1d164e"
       ],
       "layout": "IPY_MODEL_b3ff4ad056f548489056f70819dd92bb"
      }
     }
    },
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
